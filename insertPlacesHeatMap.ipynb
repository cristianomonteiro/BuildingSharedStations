{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg\n",
    "import psycopg2.extras as pgExtras\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {  'user': 'cristiano', 'password': 'cristiano',\n",
    "            'host':'localhost', 'port':'5432', 'database':'afterqualifying'}\n",
    "\n",
    "def cleanTable():\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = pg.connect(**params)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        cur.execute('delete from PLACE_HEATMAP')\n",
    "        \n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "\n",
    "        print('Heatmap table cleaned.')\n",
    "\n",
    "    except(Exception, pg.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "def updatePlaceHeatmapStations():\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = pg.connect(**params)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        cur.execute(''' update\tPLACE_HEATMAP\n",
    "\n",
    "                        set\t\tGEOM = ST_LineInterpolatePoint(EDGE.GEOM, STATION.POSITIONINEDGE),\n",
    "                                IDEDGE_FK = EDGE.IDEDGE,\n",
    "                                POSITIONINEDGE = STATION.POSITIONINEDGE\n",
    "\n",
    "                        from\tSTREETSEGMENT EDGE, STATION\n",
    "\n",
    "                        where\tEDGE.IDEDGE = STATION.IDEDGE_FK and\n",
    "                                STATION.IDSTATION = PLACE_HEATMAP.IDPLACE and\n",
    "                                PLACE_HEATMAP.TYPEOFPLACE = 'STATION'   ''')\n",
    "\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "\n",
    "        print('Heatmap Stations updated.')\n",
    "    \n",
    "    except(Exception, pg.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    \n",
    "def updatePlaceHeatmapTrips():\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = pg.connect(**params)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        cur.execute(''' update\tPLACE_HEATMAP\n",
    "\n",
    "                        set\t\tGEOM = PLACE.GEOM,\n",
    "                                IDEDGE_FK = PLACE.IDEDGE_FK,\n",
    "                                POSITIONINEDGE = PLACE.POSITIONINEDGE\n",
    "\n",
    "                        from\tPLACE\n",
    "\n",
    "                        where\tPLACE_HEATMAP.IDPLACE = PLACE.IDPLACE and\n",
    "                                PLACE_HEATMAP.TYPEOFPLACE = 'TRIP'   ''')\n",
    "\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        \n",
    "        print('Heatmap Trips updated.')\n",
    "    \n",
    "    except(Exception, pg.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reopenConn(conn, cur, params):\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    conn = pg.connect(**params)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    return conn, cur\n",
    "\n",
    "class Place:\n",
    "    currentDescription = None\n",
    "\n",
    "    def __init__(self, idPlace, edgeId, positionInEdge, valueHeatMap):\n",
    "        self.idPlace = idPlace\n",
    "        self.edgeId = edgeId\n",
    "        self.positionInEdge = positionInEdge\n",
    "        self.valueHeatMap = valueHeatMap\n",
    "\n",
    "def loadStationsNumberVehiclesParkingSlots(fileName):\n",
    "    places = list()\n",
    "\n",
    "    with open(fileName) as jsonFile:\n",
    "        optimalSolution = json.load(jsonFile)\n",
    "\n",
    "        for var in optimalSolution[\"Vars\"]:\n",
    "            varVTag = var[\"VTag\"][0]\n",
    "            varVTagSplit = varVTag.split('_')\n",
    "\n",
    "            if varVTag.startswith('station') and varVTag.endswith('start'):\n",
    "                amountVehicles = int(var[\"X\"])\n",
    "                if amountVehicles > 0:\n",
    "                    idStation = int(varVTagSplit[1])\n",
    "                    place = Place(idStation, None, None, amountVehicles)\n",
    "                    places.append(place)\n",
    "\n",
    "    return places\n",
    "\n",
    "def storeStations(fileName, description):\n",
    "    Place.currentDescription = description\n",
    "    places = loadStationsNumberVehiclesParkingSlots(fileName)\n",
    "    \n",
    "    conn = None\n",
    "    try:\n",
    "        conn = pg.connect(**params)\n",
    "        cur = conn.cursor()\n",
    "                \n",
    "        lengthToInsert = 100000\n",
    "        placesToInsert = list()\n",
    "        sqlInsert = 'insert into PLACE_HEATMAP (IDPLACE, DESCRIPTION, VALUEHEATMAP, TYPEOFPLACE) values %s'\n",
    "        for place in places:\n",
    "            placesToInsert.append((place.idPlace, 'Initial Vehicles ' + Place.currentDescription, place.valueHeatMap, 'STATION'))\n",
    "            \n",
    "            if len(placesToInsert) > lengthToInsert:\n",
    "                pgExtras.execute_values(cur, sqlInsert, placesToInsert)\n",
    "                placesToInsert = []\n",
    "                conn, cur = reopenConn(conn, cur, params)\n",
    "        \n",
    "        if len(placesToInsert) > 0:\n",
    "            pgExtras.execute_values(cur, sqlInsert, placesToInsert)\n",
    "\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "\n",
    "        print(description + ' inserted into Heatmap.')\n",
    "\n",
    "    except(Exception, pg.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trip:\n",
    "    #Values for defining the new timestamps\n",
    "    ADJUSTED_MONTH = 1\n",
    "    ADJUSTED_YEAR = 2017\n",
    "\n",
    "    def adjustDay(timeDepartureOld, drivingDuration):\n",
    "        #The weekday() starts from 0 but it does not exist a day 0 in the calendar. Then, a +1 solves this issue\n",
    "        adjustedDayDeparture = timeDepartureOld.weekday() + 1\n",
    "\n",
    "        timeDepartureNew = timeDepartureOld.replace(day=adjustedDayDeparture, month=Trip.ADJUSTED_MONTH, year=Trip.ADJUSTED_YEAR)\n",
    "        timeArrivalNew = timeDepartureNew + timedelta(minutes=drivingDuration)\n",
    "        \n",
    "        return timeDepartureNew, timeArrivalNew\n",
    "\n",
    "    def __init__(self, idTrip, expansionFactor, placeStart, placeEnd, timestampDeparture, timestampArrival, drivingDistance, drivingDuration):\n",
    "        self.idTrip = idTrip\n",
    "        self.expansionFactor = expansionFactor\n",
    "        self.placeStart = placeStart\n",
    "        self.placeEnd = placeEnd\n",
    "        self.timestampDepartureOld = timestampDeparture\n",
    "        self.timestampArrivalOld = timestampArrival\n",
    "        self.drivingDistance = drivingDistance\n",
    "        self.drivingDuration = drivingDuration\n",
    "\n",
    "        self.timestampDeparture, self.timestampArrival = Trip.adjustDay(timestampDeparture, self.drivingDuration)\n",
    "\n",
    "def idPlaceToInt(idPlace):\n",
    "    intIdPlace = int(idPlace.replace('place_', ''))\n",
    "\n",
    "    return intIdPlace\n",
    "\n",
    "def loadTrips():\n",
    "    conn = pg.connect(**params)\n",
    "\n",
    "    sqlQuery = '''\tselect\t'trip_'||TRIP.IDTRIP::text as IDTRIP,\n",
    "                            TRIP.TRIPEXPANSIONFACTOR,\n",
    "                            'place_'||TRIP.IDPLACEDEPARTURE::text as IDPLACEDEPARTURE,\n",
    "                            'place_'||TRIP.IDPLACEDESTINATION::text as IDPLACEDESTINATION,\n",
    "                            TRIP.TIMESTAMPDEPARTURE,\n",
    "                            TRIP.TIMESTAMPARRIVAL,\n",
    "                            TRIP.DRIVINGDISTANCE,\n",
    "                            TRIP.DRIVINGDURATION\n",
    "                    from\tTRIP\n",
    "                    where   TRIP.DRIVINGDISTANCE > 500\n",
    "                    '''\n",
    "    dataFrameEdges = pd.read_sql_query(sqlQuery, conn)\n",
    "    conn.close()\n",
    "\n",
    "    trips = dict()\n",
    "    for row in dataFrameEdges.itertuples():\n",
    "        dictRow = row._asdict()\n",
    "        trip = Trip(idTrip=dictRow['idtrip'],\n",
    "                    expansionFactor=dictRow['tripexpansionfactor'],\n",
    "                    placeStart=idPlaceToInt(dictRow['idplacedeparture']),\n",
    "                    placeEnd=idPlaceToInt(dictRow['idplacedestination']),\n",
    "                    timestampDeparture=dictRow['timestampdeparture'],\n",
    "                    timestampArrival=dictRow['timestamparrival'],\n",
    "                    drivingDistance=dictRow['drivingdistance'],\n",
    "                    drivingDuration=dictRow['drivingduration'])\n",
    "        trips[trip.idTrip] = trip\n",
    "\n",
    "    return trips\n",
    "\n",
    "def loadTripsServedStartEnd(fileName, tripsDatabase):\n",
    "    tripsStart = list()\n",
    "    tripsEnd = list()\n",
    "\n",
    "    with open(fileName) as jsonFile:\n",
    "        optimalSolution = json.load(jsonFile)\n",
    "\n",
    "        for var in optimalSolution[\"Vars\"]:\n",
    "            varVTag = var[\"VTag\"][0]\n",
    "            varVTagSplit = varVTag.split('_')\n",
    "\n",
    "            if varVTag.startswith('trip'):\n",
    "                idTrip = 'trip_' + varVTagSplit[1]\n",
    "                amountServed = int(var[\"X\"])\n",
    "                if amountServed > 0:\n",
    "                    trip = tripsDatabase[idTrip]\n",
    "                    trip.valueHeatMap = amountServed\n",
    "                    if varVTagSplit[2] == 'start':\n",
    "                        tripsStart.append(trip)\n",
    "                    elif varVTagSplit[2] == 'end':\n",
    "                        tripsEnd.append(trip)\n",
    "                    \n",
    "    return tripsStart, tripsEnd\n",
    "\n",
    "def storeTrips(fileName, description, tripsDatabase):\n",
    "    Place.currentDescription = description\n",
    "    tripsStart, tripsEnd = loadTripsServedStartEnd(fileName, tripsDatabase)\n",
    "    \n",
    "    conn = None\n",
    "    try:\n",
    "        conn = pg.connect(**params)\n",
    "        cur = conn.cursor()\n",
    "                \n",
    "        lengthToInsert = 100000\n",
    "        placesToInsert = list()\n",
    "        sqlInsert = 'insert into PLACE_HEATMAP (IDPLACE, DESCRIPTION, VALUEHEATMAP, TYPEOFPLACE) values %s'\n",
    "        for trip in tripsStart:\n",
    "            placesToInsert.append((trip.placeStart, 'Start ' + Place.currentDescription, trip.valueHeatMap, 'TRIP'))\n",
    "        \n",
    "        for trip in tripsEnd:\n",
    "            placesToInsert.append((trip.placeEnd, 'End ' + Place.currentDescription, trip.valueHeatMap, 'TRIP'))\n",
    "\n",
    "            if len(placesToInsert) > lengthToInsert:\n",
    "                pgExtras.execute_values(cur, sqlInsert, placesToInsert)\n",
    "                placesToInsert = []\n",
    "                conn, cur = reopenConn(conn, cur, params)\n",
    "        \n",
    "        if len(placesToInsert) > 0:\n",
    "            pgExtras.execute_values(cur, sqlInsert, placesToInsert)\n",
    "\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "\n",
    "        print(description + ' inserted into Heatmap.')\n",
    "    \n",
    "    except(Exception, pg.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBalanceData(fileName, tripsDatabase):\n",
    "    stationsStart = dict()\n",
    "    stationsBalance = list()\n",
    "    allocatedStations = 0\n",
    "    with open(fileName) as jsonFile:\n",
    "        optimalSolution = json.load(jsonFile)\n",
    "\n",
    "        for var in optimalSolution[\"Vars\"]:\n",
    "            varVTag = var[\"VTag\"][0]\n",
    "            varVTagSplit = varVTag.split('_')\n",
    "\n",
    "            if varVTag.startswith('station') and varVTag.endswith('start'):\n",
    "                amountVehicles = int(var[\"X\"])\n",
    "                if amountVehicles > 0:\n",
    "                    allocatedStations += amountVehicles\n",
    "\n",
    "                    idStation = int(varVTagSplit[1])\n",
    "                    stationsStart[idStation] = amountVehicles\n",
    "                    #stationsBalance.append((idStation, INITIAL_TIMESTAMP, amountVehicles))\n",
    "\n",
    "            elif varVTag.startswith('trip'):\n",
    "                idTrip = 'trip_' + varVTagSplit[1]\n",
    "                if varVTagSplit[2] in ['start', 'end']:\n",
    "                    idStation = int(varVTagSplit[4])\n",
    "                    vehiclesFlow = int(var[\"X\"])\n",
    "                    if varVTagSplit[2] == 'start':\n",
    "                        stationsBalance.append((idStation, tripsDatabase[idTrip].timestampDeparture, -1*vehiclesFlow))\n",
    "                    else:\n",
    "                        stationsBalance.append((idStation, tripsDatabase[idTrip].timestampArrival, vehiclesFlow))\n",
    "    \n",
    "    return stationsBalance, stationsStart\n",
    "\n",
    "def calculateSurpluses(fileName, tripsDatabase):\n",
    "    surpluses, stationsStart = loadBalanceData(fileName, tripsDatabase)\n",
    "\n",
    "    surpluses.sort()\n",
    "    i = 0\n",
    "    while i < len(surpluses):\n",
    "        currentStation = surpluses[i][0]\n",
    "        if currentStation not in stationsStart:\n",
    "            stationsStart[currentStation] = 0\n",
    "\n",
    "        earlierBalance = stationsStart[currentStation]\n",
    "\n",
    "        while i < len(surpluses) and currentStation == surpluses[i][0]:\n",
    "            #Initially, idStation comes first in the tuple sequence to keep the sorting rationale\n",
    "            (idStation, timestamp, vehiclesFlow) = surpluses[i]\n",
    "            surpluses[i] = (timestamp, idStation, earlierBalance - stationsStart[currentStation], earlierBalance)\n",
    "            earlierBalance += vehiclesFlow\n",
    "\n",
    "            i += 1\n",
    "    #Repeating this line because there is no do_while loop in Python\n",
    "    surpluses[i-1] = (timestamp, idStation, earlierBalance - stationsStart[currentStation], earlierBalance)\n",
    "\n",
    "    return surpluses\n",
    "\n",
    "def mergeExtremePoints(dataFrame, minOrMax):\n",
    "    #Fixing pandas weekday to start at Sunday, as matplotlib and datetime uses\n",
    "    dataFrame['weekday'] = dataFrame['timestamp'].dt.weekday\n",
    "    dataFrame['weekday'] = np.where(dataFrame['weekday'] <= 5, dataFrame['weekday'] + 1, 0)\n",
    "\n",
    "    if minOrMax == 'min':\n",
    "        points = dataFrame.groupby(dataFrame['weekday'])[['calcbalance']].min()\n",
    "    elif minOrMax == 'max':\n",
    "        points = dataFrame.groupby(dataFrame['weekday'])[['calcbalance']].max()\n",
    "\n",
    "    points.reset_index(inplace=True)\n",
    "    points = pd.merge(points, dataFrame, on=['weekday', 'calcbalance'])\n",
    "    points = points.groupby(points['weekday']).first()\n",
    "\n",
    "    return points\n",
    "\n",
    "def buildSurplusesDataFrames(surpluses, weekDay):\n",
    "    dataFramePd = pd.DataFrame(surpluses, columns=['timestamp', 'station', 'calcbalance', 'balance'])\n",
    "    dataFramePd = dataFramePd.set_index('timestamp')\n",
    "    dataFramePd['timestamp'] = dataFramePd.index\n",
    "    #print(dataFramePd.describe())\n",
    "\n",
    "    #Adjust the last points to the beginning of the first week\n",
    "    INITIAL_TIMESTAMP = datetime(year=Trip.ADJUSTED_YEAR, month=Trip.ADJUSTED_MONTH, day=1, hour=0, minute=0, second=0)\n",
    "    mask = dataFramePd['timestamp'] >= INITIAL_TIMESTAMP + timedelta(weeks=1)\n",
    "    dataFramePd.loc[mask, 'timestamp'] = dataFramePd.loc[mask, 'timestamp'].apply(lambda dateTooLate: dateTooLate - timedelta(weeks=1))\n",
    "\n",
    "    #dataFramePd['weekday'] = dataFramePd['timestamp'].dt.weekday\n",
    "    #dataFramePd = dataFramePd.loc[dataFramePd['weekday'] == weekDay]\n",
    "\n",
    "    #minPoints = mergeExtremePoints(dataFrame=dataFramePd, minOrMax='min')\n",
    "    maxPoints = mergeExtremePoints(dataFrame=dataFramePd, minOrMax='max')\n",
    "    #redPoints = pd.concat([minPoints, maxPoints])\n",
    "\n",
    "    #Selecting only common surpluses about the time when the maximum happened\n",
    "    timestampMaxWeekDay = maxPoints.loc[weekDay, 'timestamp']\n",
    "    dataFramePd = dataFramePd.loc[dataFramePd['timestamp'] <= timestampMaxWeekDay]\n",
    "    groupByDF = dataFramePd.groupby(dataFramePd['station'])[['timestamp']].max()\n",
    "    dataFramePd.reset_index(inplace=True, drop=True)\n",
    "    dataFramePd = pd.merge(dataFramePd, groupByDF, on=['station', 'timestamp'])\n",
    "\n",
    "    return dataFramePd#, redPoints\n",
    "\n",
    "def storeMidnightSurpluses(description, surpluses, weekDay):\n",
    "    Place.currentDescription = description\n",
    "    dataFramePd = buildSurplusesDataFrames(surpluses=surpluses, weekDay=weekDay)\n",
    "    #Useful for using the strftime to get the weekday name\n",
    "    fooTimestampWeekDay = datetime(year=Trip.ADJUSTED_YEAR, month=Trip.ADJUSTED_MONTH, day=weekDay + 1, hour=0, minute=0, second=0)\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = pg.connect(**params)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        lengthToInsert = 100000\n",
    "        placesToInsert = list()\n",
    "        sqlInsert = 'insert into PLACE_HEATMAP (IDPLACE, DESCRIPTION, VALUEHEATMAP, TYPEOFPLACE) values %s'\n",
    "\n",
    "        for index, row in dataFramePd.iterrows():\n",
    "            placesToInsert.append((row['station'], 'Surplus ' + fooTimestampWeekDay.strftime('%A') + ' ' + Place.currentDescription, row['calcbalance'], 'STATION'))\n",
    "\n",
    "            if len(placesToInsert) > lengthToInsert:\n",
    "                pgExtras.execute_values(cur, sqlInsert, placesToInsert)\n",
    "                placesToInsert = []\n",
    "                conn, cur = reopenConn(conn, cur, params)\n",
    "        \n",
    "        if len(placesToInsert) > 0:\n",
    "            pgExtras.execute_values(cur, sqlInsert, placesToInsert)\n",
    "\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "\n",
    "        print('Surpluses ' + fooTimestampWeekDay.strftime('%A') + ' ' + description + ' inserted into Heatmap.')\n",
    "    \n",
    "    except(Exception, pg.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmap table cleaned.\n",
      "Free Floating 4000 km 500 m 0.7 inserted into Heatmap.\n",
      "Free Floating 4000 km 500 m 2 inserted into Heatmap.\n",
      "Free Floating 3000 km 500 m 0.7 inserted into Heatmap.\n",
      "Mixed Free Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Restricted 1 31 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Restricted 1 62 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Restricted 1 inf Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Free Floating 4000 km 500 m 0.7 inserted into Heatmap.\n",
      "Free Floating 4000 km 500 m 2 inserted into Heatmap.\n",
      "Free Floating 3000 km 500 m 0.7 inserted into Heatmap.\n",
      "Mixed Free Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Restricted 1 31 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Restricted 1 62 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Restricted 1 inf Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Sunday Free Floating 4000 km 500 m 0.7 inserted into Heatmap.\n",
      "Surpluses Monday Free Floating 4000 km 500 m 0.7 inserted into Heatmap.\n",
      "Surpluses Tuesday Free Floating 4000 km 500 m 0.7 inserted into Heatmap.\n",
      "Surpluses Wednesday Free Floating 4000 km 500 m 0.7 inserted into Heatmap.\n",
      "Surpluses Thursday Free Floating 4000 km 500 m 0.7 inserted into Heatmap.\n",
      "Surpluses Friday Free Floating 4000 km 500 m 0.7 inserted into Heatmap.\n",
      "Surpluses Saturday Free Floating 4000 km 500 m 0.7 inserted into Heatmap.\n",
      "Surpluses Sunday Free Floating 4000 km 500 m 2 inserted into Heatmap.\n",
      "Surpluses Monday Free Floating 4000 km 500 m 2 inserted into Heatmap.\n",
      "Surpluses Tuesday Free Floating 4000 km 500 m 2 inserted into Heatmap.\n",
      "Surpluses Wednesday Free Floating 4000 km 500 m 2 inserted into Heatmap.\n",
      "Surpluses Thursday Free Floating 4000 km 500 m 2 inserted into Heatmap.\n",
      "Surpluses Friday Free Floating 4000 km 500 m 2 inserted into Heatmap.\n",
      "Surpluses Saturday Free Floating 4000 km 500 m 2 inserted into Heatmap.\n",
      "Surpluses Sunday Free Floating 3000 km 500 m 0.7 inserted into Heatmap.\n",
      "Surpluses Monday Free Floating 3000 km 500 m 0.7 inserted into Heatmap.\n",
      "Surpluses Tuesday Free Floating 3000 km 500 m 0.7 inserted into Heatmap.\n",
      "Surpluses Wednesday Free Floating 3000 km 500 m 0.7 inserted into Heatmap.\n",
      "Surpluses Thursday Free Floating 3000 km 500 m 0.7 inserted into Heatmap.\n",
      "Surpluses Friday Free Floating 3000 km 500 m 0.7 inserted into Heatmap.\n",
      "Surpluses Saturday Free Floating 3000 km 500 m 0.7 inserted into Heatmap.\n",
      "Surpluses Sunday Mixed Free Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Monday Mixed Free Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Tuesday Mixed Free Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Wednesday Mixed Free Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Thursday Mixed Free Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Friday Mixed Free Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Saturday Mixed Free Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Sunday Restricted 1 31 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Monday Restricted 1 31 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Tuesday Restricted 1 31 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Wednesday Restricted 1 31 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Thursday Restricted 1 31 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Friday Restricted 1 31 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Saturday Restricted 1 31 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Sunday Restricted 1 62 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Monday Restricted 1 62 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Tuesday Restricted 1 62 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Wednesday Restricted 1 62 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Thursday Restricted 1 62 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Friday Restricted 1 62 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Saturday Restricted 1 62 Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Sunday Restricted 1 inf Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Monday Restricted 1 inf Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Tuesday Restricted 1 inf Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Wednesday Restricted 1 inf Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Thursday Restricted 1 inf Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Friday Restricted 1 inf Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Surpluses Saturday Restricted 1 inf Partial Floating 3000 km 500 m 0.9 inserted into Heatmap.\n",
      "Heatmap Stations updated.\n",
      "Heatmap Trips updated.\n"
     ]
    }
   ],
   "source": [
    "cleanTable()\n",
    "\n",
    "folder = 'Optimal Solutions/'\n",
    "filenamesDescriptions = [   (folder + 'Free Floating/4000/500/70.json', 'Free Floating 4000 km 500 m 0.7'),\n",
    "                            (folder + 'Free Floating/4000/500/200.json', 'Free Floating 4000 km 500 m 2'),\n",
    "                            (folder + 'Free Floating/3000/500/70.json', 'Free Floating 3000 km 500 m 0.7'),\n",
    "                            (folder + 'Mixed Free Floating/3000/500/90.json', 'Mixed Free Floating 3000 km 500 m 0.9'),\n",
    "                            (folder + 'Restricted 1 31 Partial Floating/3000/500/90.json', 'Restricted 1 31 Partial Floating 3000 km 500 m 0.9'),\n",
    "                            (folder + 'Restricted 1 62 Partial Floating/3000/500/90.json', 'Restricted 1 62 Partial Floating 3000 km 500 m 0.9'),\n",
    "                            (folder + 'Restricted 1 inf Partial Floating/3000/500/90.json', 'Restricted 1 inf Partial Floating 3000 km 500 m 0.9')\n",
    "                        ]\n",
    "\n",
    "for filename, description in filenamesDescriptions:\n",
    "    storeStations(  fileName=filename,\n",
    "                    description=description)\n",
    "\n",
    "tripsDatabase = loadTrips()\n",
    "for filename, description in filenamesDescriptions:\n",
    "    storeTrips( fileName=filename,\n",
    "                description=description,\n",
    "                tripsDatabase=tripsDatabase)\n",
    "\n",
    "for filename, description in filenamesDescriptions:\n",
    "    surpluses = calculateSurpluses( fileName=filename,\n",
    "                                    tripsDatabase=tripsDatabase)\n",
    "    for weekDay in range(7):\n",
    "        storeMidnightSurpluses( description=description,\n",
    "                                surpluses=surpluses,\n",
    "                                weekDay=weekDay)\n",
    "\n",
    "updatePlaceHeatmapStations()\n",
    "updatePlaceHeatmapTrips()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b04396ebd4f74f170fc7886f30f673a7231b611b6a244b83ebde391c4c23e86b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
